# ğŸ§  LangGraph â€” Learning Project

> A hands-on learning project for building **intelligent, stateful, multi-step LLM workflows** using [LangGraph](https://langchain-ai.github.io/langgraph/).

---

## ğŸ“– Table of Contents

- [What is LangGraph?](#-what-is-langgraph)
- [Why LangGraph?](#-why-langgraph)
- [Key Definitions](#-key-definitions)
- [LLM Workflows](#-llm-workflows)
- [Graphs, Nodes & Edges](#-graphs-nodes--edges)
- [State](#-state)
- [Project Structure](#-project-structure)
- [What We Built](#-what-we-built)
- [Tech Stack](#-tech-stack)
- [Prerequisites](#-prerequisites)
- [Installation & Setup](#-installation--setup)
- [How to Run](#-how-to-run)
- [Example Graph Flow](#-example-graph-flow)
- [Common Errors & Fixes](#-common-errors--fixes)
- [Resources](#-resources)

---

## ğŸ¤” What is LangGraph?

**LangGraph** is an orchestration framework for building intelligent, stateful, and multi-step LLM workflows. Instead of chaining prompts in a simple linear sequence, LangGraph lets you model your logic as a **graph of nodes (tasks) and edges (routing)**.

It enables advanced features like:
- âš¡ **Parallelism** â€” Run multiple tasks at the same time
- ğŸ” **Loops** â€” Retry or iterate until a condition is met
- ğŸ”€ **Branching** â€” Route to different nodes based on conditions
- ğŸ§  **Memory** â€” Persist state across conversations
- â¸ï¸ **Resumability** â€” Pause and resume workflows

This makes it ideal for **agentic** and **production-grade AI applications**.

---

## ğŸ’¡ Why LangGraph?

| Feature | Simple Chain | LangGraph |
|---------|-------------|-----------|
| Linear execution | âœ… | âœ… |
| Parallel execution | âŒ | âœ… |
| Conditional routing | âŒ | âœ… |
| Loops & retries | âŒ | âœ… |
| State management | âŒ | âœ… |
| Conversation memory | âŒ | âœ… |
| Production-ready | âŒ | âœ… |

---

## ğŸ“š Key Definitions

### ğŸ”¹ LLM (Large Language Model)
A Large Language Model is a type of AI model trained on massive amounts of text data. It can understand and generate human-like text. Examples: GPT-4, Llama 3, Gemini, Claude.

### ğŸ”¹ LangChain
LangChain is a Python framework for building applications powered by LLMs. It provides tools for prompts, chains, memory, tools, and agents. LangGraph is built on top of LangChain.

### ğŸ”¹ Groq
Groq is an AI inference company that provides ultra-fast LLM API access. We use their API to run the `llama-3.3-70b-versatile` model in this project.

### ğŸ”¹ Pydantic
Pydantic is a data validation library for Python. We use Pydantic `BaseModel` to define structured output schemas â€” forcing the LLM to return data in a specific format (e.g., `feedback: str`, `score: int`).

### ğŸ”¹ StateGraph
The main class in LangGraph. You define your state type, add nodes (functions), connect them with edges, and compile the graph into a runnable application.

### ğŸ”¹ Checkpointer (MemorySaver)
A checkpointer saves the state of the graph after each node execution. `MemorySaver` is an in-memory checkpointer â€” it enables conversation memory (multi-turn chat) by persisting state between invocations.

---

## ğŸ”„ LLM Workflows

LLM workflows are a step-by-step process using which we can build complex LLM applications or a series of tasks to achieve a goal.

Each step in a workflow performs a distinct task â€” such as prompting, reasoning, tool calling, memory access, or decision-making.

Workflows can be **linear**, **parallel**, **branched**, or **looped**.

### Common Workflow Patterns

#### 1. ğŸ”— Prompt Chaining
- **What**: Breaking a complex task into a sequence of simpler prompts where each prompt uses the previous prompt's output as input.
- **How**: Solve one subtask at a time â†’ pass its result to the next prompt â†’ compose final output from intermediate results.
- **Why**: Improves reliability, control, and traceability for complex requests.
- **Example in this project**: `Sequential Workflow 2.py` â€” Generate outline â†’ Write draft â†’ Polish & summarize.

#### 2. ğŸš¦ Routing
- **What**: Direct each incoming task/request to the correct handler or service based on rules, metadata, or content.
- **How**: Incoming message â†’ identify intent â†’ route to the right handler â†’ return response.
- **Why**: Different types of inputs often need different processing logic.

#### 3. âš¡ Parallelization
- **What**: Run multiple independent LLM calls or tasks **at the same time** to reduce latency and increase throughput.
- **How**: For a given task, break it into multiple subtasks, execute all subtasks concurrently, and then merge their results.
- **Why**: When tasks are independent of each other, running them in parallel is significantly faster.
- **Example in this project**: `Parallel Workflow 2.py` â€” Evaluate language, analysis, and clarity simultaneously.

#### 4. ğŸ¯ Orchestrator Workers
- **What**: Components that schedule, execute, and monitor workflow nodes, coordinating concurrent runs, retries, and state transitions.
- **How**: Orchestrator dispatches workers â†’ each worker runs a task â†’ reports results back â†’ orchestrator merges and advances.
- **Why**: Enables scalable, fault-tolerant workflow execution.

#### 5. ğŸ“Š Evaluator Optimizer
- **What**: A component that scores, ranks, and selects candidate outputs to produce the best final result.
- **How**: Generate multiple candidate responses â†’ score each using metrics â†’ choose or ensemble the best.
- **Why**: Improves output quality by selecting from multiple attempts.

---

## ğŸ—ï¸ Graphs, Nodes & Edges

### 1. Graph
A **Graph** is the overarching structure that maps out how different tasks (nodes) are connected and executed. It visually represents the workflow, showing the sequence and conditional paths between various operations.

> ğŸ—ºï¸ **Analogy**: A road map displaying different routes connecting cities, with intersections offering choices on which path to take next.

### 2. Node
**Nodes** are individual functions or operations that perform specific tasks within the graph. Each node receives input (the current state), processes it, and produces an output (updated state).

> ğŸ­ **Analogy**: Assembly line stations â€” each station does one job: attach a part, paint it, inspect quality, and so on.

**Important Rules:**
- Every node function receives the current state as input.
- Every node function **must return a `dict`** (partial state update) â€” never return raw values like `float` or `str`.
- Nodes can contain any Python logic â€” LLM calls, computations, API calls, etc.

### 3. Edges
**Edges** are the connections between nodes that determine the flow of execution. They tell us which node should be executed next after the current one completes.

> ğŸ›¤ï¸ **Analogy**: Train tracks connecting stations together in a specific direction.

**Types of Edges:**
```python
# Normal edge â€” A always goes to B
graph.add_edge('node_a', 'node_b')

# Fan-out â€” START goes to multiple nodes (parallel execution)
graph.add_edge(START, 'node_a')
graph.add_edge(START, 'node_b')
graph.add_edge(START, 'node_c')

# Fan-in â€” Multiple nodes go to one aggregator
graph.add_edge('node_a', 'aggregator')
graph.add_edge('node_b', 'aggregator')
graph.add_edge('node_c', 'aggregator')
```

---

## ğŸ“¦ State

The **State** is a shared data structure that holds the current information or context of the entire application. In simple terms, it is the application's memory â€” keeping track of variables and data that nodes can access and modify as they execute.

> ğŸ“‹ **Analogy**: A whiteboard in a meeting room â€” participants (nodes) write and read information on the whiteboard (state) to stay updated and coordinate actions.

### Defining State

State is typically defined using Python's `TypedDict`:

```python
from typing import TypedDict

class MyState(TypedDict):
    query: str
    response: str
    score: float
```

### State Reducers (for Parallel Workflows)

When multiple parallel nodes write to the same state key, you need a **reducer** to define how to combine the values:

```python
from typing import Annotated, List
import operator

class MyState(TypedDict):
    # operator.add means: APPEND to the list instead of replacing
    scores: Annotated[List[int], operator.add]
```

Without a reducer, the last node to finish would overwrite the value from previous nodes.

---

## ğŸ“‚ Project Structure

```
LangGraph/
â”œâ”€â”€ ğŸ“„ README.md                    â† You are here
â”œâ”€â”€ ğŸ“„ requirements.txt             â† Python dependencies
â”œâ”€â”€ ğŸ“„ .env                         â† Environment variables (GROQ_API_KEY)
â”œâ”€â”€ ğŸ“„ .gitignore                   â† Git ignore rules
â”‚
â”œâ”€â”€ ğŸ“ Basic Bot/
â”‚   â”œâ”€â”€ ğŸ“„ ChatBot.py               â† Interactive chatbot with memory
â”‚   â””â”€â”€ ğŸ“„ README.md                â† Docs for this section
â”‚
â”œâ”€â”€ ğŸ“ Sequential Workflow/
â”‚   â”œâ”€â”€ ğŸ“„ Sequential Workflow 1.py â† Simple single-node LLM query
â”‚   â”œâ”€â”€ ğŸ“„ Sequential Workflow.py   â† BMI Calculator (2 nodes)
â”‚   â”œâ”€â”€ ğŸ“„ Sequential Workflow 2.py â† Blog Post Generator (prompt chaining, 3 nodes)
â”‚   â””â”€â”€ ğŸ“„ README.md                â† Docs for this section
â”‚
â”œâ”€â”€ ğŸ“ Parallel Workflow/
â”‚   â”œâ”€â”€ ğŸ“„ Parallel Workflow.py     â† Cricket Player Stats (3 parallel nodes)
â”‚   â”œâ”€â”€ ğŸ“„ Parallel Workflow 1.py   â† Structured LLM Output with Pydantic
â”‚   â”œâ”€â”€ ğŸ“„ Parallel Workflow 2.py   â† UPSE Essay Evaluator (parallel LLM evaluation)
â”‚   â””â”€â”€ ğŸ“„ README.md                â† Docs for this section
â”‚
â””â”€â”€ ğŸ“ langvenv/                    â† Python virtual environment
```

---

## ğŸ† What We Built

### Module 1: Basic Bot â€” [`Basic Bot/`](./Basic%20Bot/)
An interactive **chatbot with conversation memory**. Uses `MemorySaver` checkpointer to remember previous messages across turns. Runs in a loop â€” type messages and get AI responses.

### Module 2: Sequential Workflow â€” [`Sequential Workflow/`](./Sequential%20Workflow/)
Three examples of **sequential (linear) workflows** where nodes execute one after another:
1. **Simple LLM Query** â€” Single node that sends a query to the LLM.
2. **BMI Calculator** â€” Two nodes: calculate BMI â†’ label the category. Pure computation, no LLM.
3. **Blog Post Generator** â€” Three-step **prompt chaining**: generate outline â†’ write draft â†’ polish & summarize.

### Module 3: Parallel Workflow â€” [`Parallel Workflow/`](./Parallel%20Workflow/)
Three examples of **parallel execution** where multiple nodes run simultaneously:
1. **Cricket Player Stats** â€” Three stats computed in parallel, then summarized.
2. **Structured LLM Output** â€” Using Pydantic `BaseModel` to force the LLM to return typed data.
3. **UPSE Essay Evaluator** â€” Three LLM evaluators run in parallel (language, analysis, clarity), then an aggregator produces the overall score.

---

## ğŸ› ï¸ Tech Stack

| Technology | Purpose |
|------------|---------|
| [Python 3.10+](https://www.python.org/) | Programming language |
| [LangGraph](https://langchain-ai.github.io/langgraph/) | Graph-based workflow orchestration |
| [LangChain](https://www.langchain.com/) | LLM framework (core, messages, tools) |
| [LangChain-Groq](https://python.langchain.com/docs/integrations/chat/groq/) | Groq LLM integration for LangChain |
| [Groq API](https://console.groq.com/) | Ultra-fast LLM inference (Llama 3.3 70B) |
| [Pydantic](https://docs.pydantic.dev/) | Data validation & structured output |
| [python-dotenv](https://pypi.org/project/python-dotenv/) | Load environment variables from `.env` file |

---

## âœ… Prerequisites

Before you begin, make sure you have:

1. **Python 3.10 or higher** installed on your machine.
   ```bash
   python --version
   ```

2. **A Groq API Key** â€” Sign up for free at [console.groq.com](https://console.groq.com/) and create an API key.

3. **Git** (optional) â€” For cloning and version control.

---

## ğŸš€ Installation & Setup

### Step 1: Clone the Repository

```bash
git clone <your-repo-url>
cd LangGraph
```

### Step 2: Create a Virtual Environment

```bash
python -m venv langvenv
```

### Step 3: Activate the Virtual Environment

**Windows (PowerShell):**
```powershell
.\langvenv\Scripts\Activate.ps1
```

**Windows (Command Prompt):**
```cmd
.\langvenv\Scripts\activate.bat
```

**Linux / macOS:**
```bash
source langvenv/bin/activate
```

### Step 4: Install Dependencies

```bash
pip install -r requirements.txt
```

This will install:
- `langchain` â€” Core LangChain framework
- `langchain-core` â€” Core abstractions (messages, prompts)
- `langchain-community` â€” Community integrations
- `langchain-groq` â€” Groq LLM integration
- `python-dotenv` â€” Environment variable management
- `langgraph` â€” Graph-based workflow orchestration
- `pydantic` â€” Data validation for structured output

### Step 5: Configure Environment Variables

Create a `.env` file in the root directory:

```env
GROQ_API_KEY=your_groq_api_key_here
```

> âš ï¸ **Never commit your `.env` file to Git.** Make sure it's listed in `.gitignore`.

---

## â–¶ï¸ How to Run

### 1. Activate the Virtual Environment

```powershell
.\langvenv\Scripts\Activate.ps1
```

### 2. Run Any Script

```bash
# Basic Bot â€” Interactive chatbot
python "Basic Bot/ChatBot.py"

# Sequential Workflow â€” Simple LLM query
python "Sequential Workflow/Sequential Workflow 1.py"

# Sequential Workflow â€” BMI Calculator
python "Sequential Workflow/Sequential Workflow.py"

# Sequential Workflow â€” Blog Post Generator (Prompt Chaining)
python "Sequential Workflow/Sequential Workflow 2.py"

# Parallel Workflow â€” Cricket Player Stats
python "Parallel Workflow/Parallel Workflow.py"

# Parallel Workflow â€” Structured LLM Output
python "Parallel Workflow/Parallel Workflow 1.py"

# Parallel Workflow â€” UPSE Essay Evaluator
python "Parallel Workflow/Parallel Workflow 2.py"
```

---

## ğŸ—ºï¸ Example Graph Flow

### UPSC Essay Evaluation System (What We Are Building Towards)

The system generates an essay topic, collects the student's submission, and evaluates it in parallel on depth of analysis, language quality, and clarity of thought. Based on the combined score, it either gives feedback for improvement or approves the essay.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GenerateTopic  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CollectEssay   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         EvaluateEssay (Parallel)            â”‚
â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚EvaluateDepth â”‚  â”‚ EvaluateLanguage  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚          â”‚ EvaluateClarity  â”‚               â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ AggregateResults â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ConditionalRoutingâ”‚
          â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
              â–¼          â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚GiveFeedbackâ”‚  â”‚ ShowSuccess  â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚CollectRevision â”‚ â”€â”€â†’ (Loop back to EvaluateEssay)
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Steps:
1. **GenerateTopic** â€” System generates a relevant UPSC-style essay topic.
2. **CollectEssay** â€” Student writes and submits the essay.
3. **EvaluateEssay** (Parallel) â€” Three evaluations run simultaneously:
   - **EvaluateDepth** â€” Analyzes depth of analysis, argument strength, critical thinking.
   - **EvaluateLanguage** â€” Checks grammar, vocabulary, fluency, and tone.
   - **EvaluateClarity** â€” Assesses coherence, logical flow, and clarity of thought.
4. **AggregateResults** â€” Combines scores and generates a total score.
5. **ConditionalRouting** â€” Routes based on score:
   - Score meets threshold â†’ **ShowSuccess** âœ…
   - Score below threshold â†’ **GiveFeedback** âŒ
6. **GiveFeedback** â€” Provides targeted suggestions for improvement.
7. **CollectRevision** (optional loop) â€” Student resubmits â†’ loop back to evaluation.
8. **ShowSuccess** â€” Congratulates the student and ends the flow.

---

## âš ï¸ Common Errors & Fixes

Here are the errors we encountered and fixed while building this project:

### 1. `ModuleNotFoundError: No module named 'langgraph'`
**Cause:** Dependencies not installed in the virtual environment.
**Fix:**
```bash
pip install -r requirements.txt
```

### 2. `InvalidUpdateError: Expected dict, got 2.0`
**Cause:** Node functions returning raw values (`float`, `str`) instead of dictionaries.
**Fix:** Always return a `dict` from node functions:
```python
# âŒ Wrong
def my_node(state):
    return 42

# âœ… Correct
def my_node(state):
    return {'score': 42}
```

### 3. `TypeError: 'ResponseState' object is not subscriptable`
**Cause:** Using bracket notation on a Pydantic object.
**Fix:** Use dot notation for Pydantic objects:
```python
# âŒ Wrong
response['feedback']

# âœ… Correct
response.feedback
```

### 4. `ImportError: cannot import name 'MemorySaver' from 'langgraph.graph.memory'`
**Cause:** Import path changed in newer versions of LangGraph.
**Fix:**
```python
# âŒ Old path
from langgraph.graph.memory import MemorySaver

# âœ… New path
from langgraph.checkpoint.memory import MemorySaver
```

### 5. Using `{}` (set) instead of `[]` (list) for prompt messages
**Cause:** Sets are unordered and not valid input for LLM invocation.
**Fix:**
```python
# âŒ Wrong â€” this creates a set (unordered!)
prompt = {
    SystemMessage(content="..."),
    HumanMessage(content="...")
}

# âœ… Correct â€” this creates a list (ordered)
prompt = [
    SystemMessage(content="..."),
    HumanMessage(content="...")
]
```

### 6. `NameError: name 'Field' is not defined`
**Cause:** Using `Field()` without importing it from Pydantic.
**Fix:**
```python
from pydantic import BaseModel, Field
```

---

## ğŸ“š Resources

- ğŸ“˜ [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- ğŸ“˜ [LangChain Documentation](https://python.langchain.com/docs/)
- ğŸ“˜ [Groq Console](https://console.groq.com/)
- ğŸ“˜ [Pydantic Documentation](https://docs.pydantic.dev/)
- ğŸ“˜ [LangGraph GitHub](https://github.com/langchain-ai/langgraph)

---

## ğŸ“ License

This is a personal learning project. Feel free to use it for your own learning!

---

> Built with â¤ï¸ using LangGraph + Groq + LangChain